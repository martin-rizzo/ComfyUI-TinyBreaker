"""
File    : load_t5_encoder_experimental.py
Purpose : Node to load the T5 encoder using experimental methods
Author  : Martin Rizzo | <martinrizzo@gmail.com>
Date    : Feb 17, 2025
Repo    : https://github.com/martin-rizzo/ComfyUI-TinyBreaker
License : MIT
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
                              ComfyUI-TinyBreaker
 ComfyUI nodes for experimenting with the capabilities of the TinyBreaker model.
  (TinyBreaker is a hybrid model that combines the strengths of PixArt and SD)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
"""
import comfy.text_encoders.t5
import comfy.model_management
import torch
from .xcomfy.clip               import CLIP
from .utils.directories         import TEXT_ENCODERS_DIR
from .core.t5_encoder_model_cmf import get_t5_encoder_custom_class
_AUTOMATIC = "auto"

_CLIP_TYPES = [
    _AUTOMATIC,
    "sd3",
    "pixart"
]
_INFERENCE_DTYPES_BY_NAME = {
    _AUTOMATIC       : None,
  # "comfyui native" : None,
    "bfloat16"       : torch.bfloat16,
    "float16"        : torch.float16,
    "float32"        : torch.float32,
}
_INFERENCE_MODE_NATIVE  = "comfyui native"
_INFERENCE_MODE_CPU     = "cpu (slow)"
_INFERENCE_MODE_GPU     = "gpu (high vram usage)"
_INFERENCE_MODE_DYNAMIC = "dynamic loading"
_INFERENCE_MODES = [ _AUTOMATIC, _INFERENCE_MODE_NATIVE, _INFERENCE_MODE_CPU, _INFERENCE_MODE_GPU, _INFERENCE_MODE_DYNAMIC ]


class LoadT5EncoderExperimental:
    TITLE       = "ðŸ’ªTB | Load T5 Encoder (Experimental)"
    CATEGORY    = "TinyBreaker"
    DESCRIPTION = "Load a T5 encoder using experimental methods to utilize limited GPU memory on mid-range or low-end GPUs."

    #__ PARAMETERS ________________________________________
    @classmethod
    def INPUT_TYPES(cls):
        return {
        "required": {
            "t5_name"        : (cls.t5_names()       , {"tooltip": "The name of the T5 encoder checkpoint to load.",
                                                       }),
            "type"           : (cls.clip_types()     , {"tooltip": "Specifies the model format in which ComfyUI processes embeddings generated by the T5 encoder.",
                                                        "default": _AUTOMATIC,
                                                       }),
            "inference_mode" : (cls.inference_modes(), {"tooltip": "Choose the code that will be used to perform inference.",
                                                        "default": _AUTOMATIC,
                                                       }),
            "inference_dtype": (cls.dtypes()         , {"tooltip": "Data type used for inference.",
                                                        "default": _AUTOMATIC,
                                                       }),
            },
        }

    #__ FUNCTION __________________________________________
    FUNCTION = "load_t5_encoder"
    RETURN_TYPES    = ("CLIP",)
    OUTPUT_TOOLTIPS = ("The loaded T5 Encoder ready for use as a CLIP connection.",)

    def load_t5_encoder(self, t5_name, type, inference_mode, inference_dtype):
        _cpu = torch.device("cpu")
        _gpu = comfy.model_management.get_torch_device()

        # resolve automatic settings
        # TODO: implement an algorithm to determine the best inference mode/dtype (?)
        if type            == _AUTOMATIC:  type            = "sd3"
        if inference_mode  == _AUTOMATIC:  inference_mode  = _INFERENCE_MODE_DYNAMIC
        if inference_dtype == _AUTOMATIC:  inference_dtype = "bfloat16"


        if inference_mode == _INFERENCE_MODE_GPU:
            # custom T5 encoder fully loaded on GPU
            store_device     = _gpu
            inference_device = _gpu
            t5_encoder_class = "custom"

        elif inference_mode == _INFERENCE_MODE_CPU:
            # custom T5 encoder fully loaded on CPU
            store_device     = _cpu
            inference_device = _cpu
            t5_encoder_class = "custom"

        elif inference_mode == _INFERENCE_MODE_DYNAMIC:
            # custom T5 encoder loaded on CPU with dynamic layer allocation on GPU
            store_device     = _cpu
            inference_device = _gpu
            t5_encoder_class = "custom"

        else:
            # by default use ComfyUI's built-in T5 code without customizations
            store_device     = None
            inference_device = None
            t5_encoder_class = comfy.text_encoders.t5.T5


        # if a custom T5 encoder is used, get the custom class
        if  t5_encoder_class == "custom":
            t5_encoder_class = get_t5_encoder_custom_class(
                                            load_device      = store_device,
                                            inference_device = inference_device,
                                            inference_dtype  = _INFERENCE_DTYPES_BY_NAME[inference_dtype],
                                            precharge_depth  = 1,
                                            )

        state_dict = TEXT_ENCODERS_DIR.load_state_dict_or_raise(t5_name)
        clip = CLIP.from_custom_t5_encoder(t5_encoder_class,
                                           state_dict,
                                           # prefix="",
                                           clip_type = type,
                                           initial_device = store_device,
                                           load_device    = store_device,
                                           offload_device = store_device,
                                           )
        return (clip,)


    #__ internal functions ________________________________

    @staticmethod
    def t5_names():
        return TEXT_ENCODERS_DIR.get_filename_list()

    @staticmethod
    def clip_types():
        return _CLIP_TYPES

    @staticmethod
    def inference_modes():
        return _INFERENCE_MODES

    @staticmethod
    def dtypes():
        return list(_INFERENCE_DTYPES_BY_NAME.keys())

