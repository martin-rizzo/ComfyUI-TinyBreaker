"""
File    : load_t5_encoder_experimental.py
Purpose : Node to load the T5 encoder using experimental methods
Author  : Martin Rizzo | <martinrizzo@gmail.com>
Date    : Feb 17, 2025
Repo    : https://github.com/martin-rizzo/ComfyUI-TinyBreaker
License : MIT
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
                              ComfyUI-TinyBreaker
 ComfyUI nodes for experimenting with the capabilities of the TinyBreaker model.
  (TinyBreaker is a hybrid model that combines the strengths of PixArt and SD)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
"""
import comfy.text_encoders.t5
import comfy.model_management
import torch
from .xcomfy.clip               import CLIP
from .utils.directories         import TEXT_ENCODERS_DIR
from .core.t5_encoder_model_cmf import get_t5_encoder_custom_class

_CLIP_TYPES = [
    "sd3",
    "pixart"
]
_INFERENCE_DTYPES_BY_NAME = {
    "default" : None,
    "bfloat16": torch.bfloat16,
    "float16" : torch.float16,
    "float32" : torch.float32,
}
_INFERENCE_MODE_NATIVE    = "comfyui native"
_INFERENCE_MODE_CPU       = "cpu (slow)"
_INFERENCE_MODE_GPU       = "gpu (high vram usage)"
_INFERENCE_MODE_OPTIMIZED = "optimized"
_INFERENCE_MODES = [ _INFERENCE_MODE_NATIVE, _INFERENCE_MODE_CPU, _INFERENCE_MODE_GPU, _INFERENCE_MODE_OPTIMIZED ]


class LoadT5EncoderExperimental:
    TITLE       = "ðŸ’ªTB | Load T5 Encoder (Experimental)"
    CATEGORY    = "TinyBreaker"
    DESCRIPTION = "Load a T5 encoder using experimental methods to utilize limited GPU memory on mid-range or low-end GPUs."

    #__ PARAMETERS ________________________________________
    @classmethod
    def INPUT_TYPES(cls):
        return {
        "required": {
            "t5_name"        : (cls.t5_names()       , {"tooltip": "The name of the T5 encoder checkpoint to load.",
                                                       }),
            "type"           : (cls.clip_types()     , {"tooltip": "Specifies the model format in which ComfyUI processes embeddings generated by the T5 encoder.",
                                                       }),
            "inference"      : (cls.inference_modes(), {"tooltip": "Choose the code that will perform inference:\n\n* \"comfyui native\": ComfyUI's built-in code.\n\n* \"optimized\": Experimental custom code to optimize memory usage.",
                                                        "default": _INFERENCE_MODE_OPTIMIZED,
                                                       }),
            "inference_dtype": (cls.dtypes()         , {"tooltip": "Data type for optimized inference.",
                                                        "default": "bfloat16",
                                                       }),
            },
        }

    #__ FUNCTION __________________________________________
    FUNCTION = "load_t5_encoder"
    RETURN_TYPES    = ("CLIP",)
    OUTPUT_TOOLTIPS = ("The loaded T5 Encoder ready for use as a CLIP connection.",)

    def load_t5_encoder(self, t5_name, type="sd3", inference="optimized", inference_dtype="bfloat16"):
        _cpu = torch.device("cpu")
        _gpu = comfy.model_management.get_torch_device()


        if inference == _INFERENCE_MODE_GPU:
            # custom T5 encoder fully loaded on GPU
            store_device     = _gpu
            inference_device = _gpu
            t5_encoder_class = "custom"

        elif inference == _INFERENCE_MODE_CPU:
            # custom T5 encoder fully loaded on CPU
            store_device     = _cpu
            inference_device = _cpu
            t5_encoder_class = "custom"

        elif inference == _INFERENCE_MODE_OPTIMIZED:
            # custom T5 encoder loaded on CPU with dynamic layer allocation on GPU
            store_device     = _cpu
            inference_device = _gpu
            t5_encoder_class = "custom"

        else:
            # by default use ComfyUI's built-in T5 code without customizations
            store_device     = None
            inference_device = None
            t5_encoder_class = comfy.text_encoders.t5.T5


        # if a custom T5 encoder is used, get the custom class
        if  t5_encoder_class == "custom":
            t5_encoder_class = get_t5_encoder_custom_class(
                                            load_device      = store_device,
                                            inference_device = inference_device,
                                            inference_dtype  = _INFERENCE_DTYPES_BY_NAME[inference_dtype],
                                            precharge_depth  = 1,
                                            )

        state_dict = TEXT_ENCODERS_DIR.load_state_dict_or_raise(t5_name)
        clip = CLIP.from_custom_t5_encoder(t5_encoder_class,
                                           state_dict,
                                           # prefix="",
                                           clip_type = type,
                                           initial_device = store_device,
                                           load_device    = store_device,
                                           offload_device = store_device,
                                           )
        return (clip,)


    #__ internal functions ________________________________

    @staticmethod
    def t5_names():
        return TEXT_ENCODERS_DIR.get_filename_list()

    @staticmethod
    def clip_types():
        return _CLIP_TYPES

    @staticmethod
    def inference_modes():
        return _INFERENCE_MODES

    @staticmethod
    def dtypes():
        return list(_INFERENCE_DTYPES_BY_NAME.keys())

